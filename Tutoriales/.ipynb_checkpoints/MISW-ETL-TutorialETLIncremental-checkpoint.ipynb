{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: creación de ETLs con PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducción\t\n",
    "\t¿Qué aprenderá?\n",
    "    En este tutorial aprenderá cómo puede usar PySpark para crear ETLs con historia en las dimensiones. \n",
    "\t\n",
    "\t¿Qué construirá? \n",
    "\tConstruirá sobre el ETL del taller anterior, esta vez incluyendo el manejo de historia para algunas de las dimensiones del modelo multidimensional.\n",
    "    \n",
    "\t¿Para qué?\n",
    "\tDentro  de  procesos  de  ETL,  es común  que se  presenten  dimensiones que puedan  presentar cambios a través del tiempo para las cuales es necesario tener un plan de manejo de historia. Por lo tanto, es esencial saber cómo realizar este manejo en las distintas herramientas de ETLs.\n",
    "    \n",
    "    ¿Qué necesita?\n",
    "    1. Python 3 con pip instalado\n",
    "    2. Jupyter notebook\n",
    "    3. Paquetes: Pyspark (3.0.1) y pandas (1.2.1)\n",
    "    4. Controlador J de MySQL\n",
    "    5. Servidor SQL con base de datos multidimensional \"WWImportersDWH\" que contenga la dimension \"stockItem_Historia\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En  este tutorial,  utilizará PySpark para crear y ejecutar un ETL con historia. Para completar este taller, es necesario que haya completado el taller anterior, creación de un ETL, pues se construirá sobre el mismo. Se presenta a continuación el modelo multidimensional sin historia, cuyos datos conforman la base que se tiene actualmente "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Modelo ordenes](./WWI_modelo_ordenes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este tutorial es transformar el modelo anterior de forma que incorpore manejo de historia de la dimensión Producto, para lo anterior se propone manterner la dimensión original como una fuente para la primera carga de datos (momento en el cual deja de ser útil) y crear una nueva para el manejo de historia (<i>ProductoHistoria</i>), se espera que despues de este proceso el modelo multidimensional sea el siguiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ETL](./ModeloHistoria.PNG)\n",
    "\n",
    "En el caso de la primera carga los valores por defecto para los atributos version, FechaInicial, FechaFinal y ID_Producto serán la fecha en la que se hace la carga, 2199-12-31 y el id de la dimension producto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ETL Incremental para Productos\n",
    "WideWorldImporters ha determinado que existe la posibilidad de que un <i>Producto</i> cambie su precio unitario. En vista de esto, se debe modificar el proceso ETL para manejar los cambios. Se ha decidido que el manejo que se le debe dar a la historia es **tipo 2**. Este tipo implica que la tabla tendrá 3 columnas nuevas 1. fecha de inicio y 2. fin de vigencia del registro y 3. version, la última es un indicador de cuántos registros hay por cada producto. Un nuevo registro es ingresado cuando se genera un cambio en alguno de los atributos, así como es necesario actualizar las 3 columnas que manejan la vigencia del Producto.\n",
    "\n",
    "De acuerdo con esto, se debe añadir una serie de  transformaciones en el ETL al final para la dimensión de Producto, es decir, en el bloque 4 del diseño del ETL, estas transformaciones incluyen las siguientes operaciones: \n",
    "<ol>\n",
    "<li>Leer los datos ya existentes en la bodega o DataWarehouse (Base de datos multidimensional Estudiante_43) y traer la última versión de cada <i>Producto</i>.</li>\n",
    "<li>Identificar de los Productos registrados en un archivo csv (<i>ProductosActualizados.csv</i>), cuáles son nuevos e insertarlos en la bodega o DataWarehouse (Base de datos multidimensional Estudiante_43) en la tabla ProductoHistoria.</li>\n",
    "<li>Para los Productos que cambiaron de precio, se debe insertar un nuevo registro con el nuevo precio, modificar el registro existente, cambiando el valor del atributo <i>Fecha final</i> y los valores de vigencia a S o N para indicar que el registro existente se ha vencido.</li>\n",
    "</ol>\n",
    "\n",
    "Primero, se cargan los datos de la base de datos transaccional esto se realiza con el fin de incorporar los nuevos datos que pudieron ser ingresados desde la última carga a la bodega multidimensional. También, se deben realizar las transformaciones regulares de la dimensión <i>Producto</i>. Para esto se mantienen los bloques diseñados en el anterior tutorial y se incorpora nuevo código en el bloque 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración servidor base de datos transaccional\n",
    "# Recuerde usar su usuario uniandes sin caracteres especiales como usuario y su codigo de estudiante como constraseña\n",
    "db_user = ''\n",
    "db_psswd = ''\n",
    "source_db_connection_string = 'jdbc:mysql://157.253.236.116:8080/WWImportersTransactional'\n",
    "\n",
    "dest_db_connection_string = 'jdbc:mysql://157.253.236.116:8080/Estudiante_43'\n",
    "\n",
    "# Driver de conexion\n",
    "path_jar_driver = 'C:\\Program Files (x86)\\MySQL\\Connector J 8.0\\mysql-connector-java-8.0.28.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pyspark.sql import functions as f, SparkSession, types as t\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.functions import udf, col, length, isnan, when, count, regexp_replace\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leidy\\Downloads\\MISO\\ambiente\\lib\\site-packages\\pyspark\\sql\\context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Configuración de la sesión\n",
    "conf=SparkConf() \\\n",
    "    .set('spark.driver.extraClassPath', path_jar_driver)\n",
    "\n",
    "spark_context = SparkContext(conf=conf)\n",
    "sql_context = SQLContext(spark_context)\n",
    "spark = sql_context.sparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conexión y carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define la función para conexión y cargue de dataframes desde la base de datos origen y luego la función para guardar un dataframe en una tabla de la base de datos destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obterner_dataframe_desde_csv(_PATH, _sep):\n",
    "    return spark.read.load(_PATH, format=\"csv\", sep=_sep, inferSchema=\"true\", header='true')\n",
    "\n",
    "def obtener_dataframe_de_bd(db_connection_string, sql, db_user, db_psswd):\n",
    "    df_bd = spark.read.format('jdbc')\\\n",
    "        .option('url', db_connection_string) \\\n",
    "        .option('dbtable', sql) \\\n",
    "        .option('user', db_user) \\\n",
    "        .option('password', db_psswd) \\\n",
    "        .option('driver', 'com.mysql.cj.jdbc.Driver') \\\n",
    "        .load()\n",
    "    return df_bd\n",
    "\n",
    "def guardar_db(db_connection_string, df, tabla, db_user, db_psswd):\n",
    "    df.select('*').write.format('jdbc') \\\n",
    "      .mode('append') \\\n",
    "      .option('url', db_connection_string) \\\n",
    "      .option('dbtable', tabla) \\\n",
    "      .option('user', db_user) \\\n",
    "      .option('password', db_psswd) \\\n",
    "      .option('driver', 'com.mysql.cj.jdbc.Driver') \\\n",
    "      .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BLOQUE 1: Dimensión <i>Empleado</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+----------+\n",
      "|ID_Empleado|            Nombre|EsVendedor|\n",
      "+-----------+------------------+----------+\n",
      "|          2|    Kayla Woodcock|      true|\n",
      "|          3|     Hudson Onslow|      true|\n",
      "|          6|     Sophia Hinton|      true|\n",
      "|          7|         Amy Trefl|      true|\n",
      "|          8|    Anthony Grosse|      true|\n",
      "|         13|Hudson Hollinworth|      true|\n",
      "|         14|         Lily Code|      true|\n",
      "|         15|         Taj Shand|      true|\n",
      "|         16|     Archer Lamble|      true|\n",
      "|         20|       Jack Potter|      true|\n",
      "+-----------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_empleados = '''(SELECT ID_persona AS ID_Empleado, NombreCompleto AS Nombre, EsVendedor FROM WWImportersTransactional.Personas WHERE EsVendedor=1) AS Temp_empleados'''\n",
    "empleados = obtener_dataframe_de_bd(source_db_connection_string, sql_empleados, db_user, db_psswd)\n",
    "empleados.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|ID_Empleado|            Nombre|\n",
      "+-----------+------------------+\n",
      "|          2|    Kayla Woodcock|\n",
      "|          3|     Hudson Onslow|\n",
      "|          6|     Sophia Hinton|\n",
      "|          7|         Amy Trefl|\n",
      "|          8|    Anthony Grosse|\n",
      "|         13|Hudson Hollinworth|\n",
      "|         14|         Lily Code|\n",
      "|         15|         Taj Shand|\n",
      "|         16|     Archer Lamble|\n",
      "|         20|       Jack Potter|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMACION\n",
    "empleados = empleados.selectExpr('ID_Empleado as ID_Empleado_T','Nombre')\n",
    "empleados = empleados.withColumn('ID_Empleado_DWH', f.monotonically_increasing_id() + 1)\n",
    "empleados.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGUE\n",
    "guardar_db(dest_db_connection_string, empleados,'Estudiante_43.Empleado', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique los resultados usando MySQL Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BLOQUE 2: Dimensión ciudad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID_ciudad', 'NombreCiudad', 'ID_EstadoProvincia', 'Poblacion'] ['ID_Pais', 'Nombre', 'Continente', 'Region', 'Subregion'] ['ID_EstadoProvincia', 'NombreEstadoProvincia', 'TerritorioVentas', 'ID_Pais']\n"
     ]
    }
   ],
   "source": [
    "#EXTRACCION\n",
    "sql_paises = '''(SELECT ID_Pais, Nombre, Continente, Region, Subregion FROM WWImportersTransactional.Paises) AS Temp_paises'''\n",
    "sql_provincias_estados = '''(SELECT ID_EstadosProvincias AS ID_EstadoProvincia, NombreEstadoProvincia, TerritorioVentas, ID_Pais FROM WWImportersTransactional.EstadosProvincias) AS Temp_estados_provincias'''\n",
    "sql_ciudades = '''(SELECT ID_ciudad as ID_ciudad_T, NombreCiudad, ID_EstadoProvincia, Poblacion FROM WWImportersTransactional.Ciudades) AS Temp_ciudades'''\n",
    "\n",
    "paises = obtener_dataframe_de_bd(source_db_connection_string, sql_paises, db_user, db_psswd)\n",
    "provincias_estados = obtener_dataframe_de_bd(source_db_connection_string, sql_provincias_estados, db_user, db_psswd)\n",
    "ciudades = obtener_dataframe_de_bd(source_db_connection_string, sql_ciudades, db_user, db_psswd)\n",
    "\n",
    "print(ciudades.columns, paises.columns, provincias_estados.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+---------+------------------+---------+---------------------+----------------+-------------+-------------+--------+----------------+\n",
      "|ID_Pais|ID_EstadoProvincia|ID_ciudad|      NombreCiudad|Poblacion|NombreEstadoProvincia|TerritorioVentas|       Nombre|   Continente|  Region|       Subregion|\n",
      "+-------+------------------+---------+------------------+---------+---------------------+----------------+-------------+-------------+--------+----------------+\n",
      "|    230|                31|       49|           Absecon|     8411|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|      150|          Adelphia|     null|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|      336|            Albion|     null|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|      458|         Allamuchy|       78|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|      480|         Allendale|     6505|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|      488|        Allenhurst|      496|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|      500|         Allentown|     1828|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|      506|         Allenwood|      925|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|      526|           Alloway|     1402|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|      574|             Alpha|     2369|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|      582|            Alpine|     1849|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|      813|          Anderson|      342|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|      835|           Andover|      606|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|     1011|           Apshawa|     null|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|     1278|            Asbury|      273|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|     1281|       Asbury Park|    16116|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|     1388|              Atco|     null|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|     1450|     Atlantic City|    39558|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|     1451|Atlantic Highlands|     4385|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "|    230|                31|     1463|            Atsion|     null|           New Jersey|         Mideast|United States|North America|Americas|Northern America|\n",
      "+-------+------------------+---------+------------------+---------+---------------------+----------------+-------------+-------------+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMACION\n",
    "ciudades = ciudades.join(provincias_estados, how = 'inner', on = 'ID_EstadoProvincia')\n",
    "ciudades = ciudades.join(paises, how = 'inner', on = 'ID_Pais')\n",
    "ciudades = ciudades.withColumn('ID_Ciudad_DWH', f.monotonically_increasing_id() + 1)\n",
    "ciudades.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGUE\n",
    "guardar_db(dest_db_connection_string, ciudades,'Estudiante_43.Ciudad', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique los resultados usando MySQL Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BLOQUE 3: Dimensión paquete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRACCION\n",
    "sql_paquetes = '''(SELECT ID_TipoPaquete AS ID_TipoPaquete_T, TipoPaquete AS Nombre FROM WWImportersTransactional.Paquetes) AS Temp_Paquetes'''\n",
    "\n",
    "paquetes = obtener_dataframe_de_bd(source_db_connection_string, sql_paquetes, db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paquetes = paquetes.withColumn('ID_TipoPaquete_DWH', f.monotonically_increasing_id() + 1)\n",
    "paquetes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGUE\n",
    "guardar_db(dest_db_connection_string, paquetes,'Estudiante_43.TipoPaquete', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique los resultados usando MySQL Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLOQUE 4: Dimensión producto\n",
    "En términos generales vamos a usar como primer carga de la nueva dimensión ProductoHistoria los datos de la dimensión Producto, simulando una fuente de datos. En un contexto real lo que se hace es modificar la misma dimensión para manejar la historia.\n",
    "\n",
    "En detalle lo que vamos a hacer es:\n",
    "1. Paso 1: Actualizar la información de la dimensión Producto\n",
    "2. Paso 2: Crear la dimensión ProductoHistoria y hacer el primer cargue en ProductoHistoria con los datos de Producto\n",
    "3. Paso 3: Guardar actualizaciones de Productos que vienen del archivo ReporteNum2_Productos.csv en ProductoHistoria según tipo 2\n",
    "\n",
    "Nota: El paso 2 sin usar pyspark serían dos pasos, sin embargo vamos a aprovechar que al guardar un dataframe se crean las tablas en la base de datos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Paso 1: Actualizar Dimensión Producto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_productos = '''(SELECT ID_Producto AS ID_Producto_T, ID_Color, NombreProducto, Marca, Necesita_refrigeracion, Dias_tiempo_entrega, Impuesto, PrecioUnitario, PrecioRecomendado FROM WWImportersTransactional.Producto) AS Temp_productos'''\n",
    "sql_colores = '''(SELECT ID_Color, Color FROM WWImportersTransactional.Colores) AS Temp_colores'''\n",
    "\n",
    "productos = obtener_dataframe_de_bd(source_db_connection_string, sql_productos, db_user, db_psswd)\n",
    "colores = obtener_dataframe_de_bd(source_db_connection_string, sql_colores, db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+-----+----------------------+-------------------+--------+--------------+-----------------+-----+\n",
      "|ID_Color|ID_Producto|      NombreProducto|Marca|Necesita_refrigeracion|Dias_tiempo_entrega|Impuesto|PrecioUnitario|PrecioRecomendado|Color|\n",
      "+--------+-----------+--------------------+-----+----------------------+-------------------+--------+--------------+-----------------+-----+\n",
      "|       3|          3|Office cube peris...| null|                     0|                 14|      15|            19|               28|Black|\n",
      "|       3|         17|DBA joke mug - mi...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         19|DBA joke mug - da...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         21|DBA joke mug - yo...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         23|DBA joke mug - it...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         25|DBA joke mug - I ...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         27|DBA joke mug - SE...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         29|DBA joke mug - tw...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         31|Developer joke mu...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         33|Developer joke mu...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         35|Developer joke mu...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         37|Developer joke mu...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         39|Developer joke mu...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         41|Developer joke mu...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         43|Developer joke mu...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         45|Developer joke mu...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         47|Developer joke mu...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         49|Developer joke mu...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         51|Developer joke mu...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "|       3|         53|IT joke mug - key...| null|                     0|                 12|      15|            13|               19|Black|\n",
      "+--------+-----------+--------------------+-----+----------------------+-------------------+--------+--------------+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMACION\n",
    "productos = productos.join(colores, how = 'inner', on = 'ID_Color')\n",
    "productos = productos.withColumn('ID_Producto_DWH', f.monotonically_increasing_id() + 1)\n",
    "productos.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGUE\n",
    "guardar_db(dest_db_connection_string, productos,'Estudiante_43.Producto', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique los resultados usando MySQL Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2: Crear y cargar la Dimensión ProductoHistoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transaformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+-----+----------------------+-------------------+--------+--------------+-----------------+-----+------------+----------+-------+\n",
      "|ID_Color|ID_Producto|      NombreProducto|Marca|Necesita_refrigeracion|Dias_tiempo_entrega|Impuesto|PrecioUnitario|PrecioRecomendado|Color|FechaInicial|FechaFinal|Vigente|\n",
      "+--------+-----------+--------------------+-----+----------------------+-------------------+--------+--------------+-----------------+-----+------------+----------+-------+\n",
      "|       3|          3|Office cube peris...| null|                     0|                 14|      15|            19|               28|Black|  2013-01-02|2199-12-31|      S|\n",
      "|       3|         17|DBA joke mug - mi...| null|                     0|                 12|      15|            13|               19|Black|  2013-01-02|2199-12-31|      S|\n",
      "|       3|         19|DBA joke mug - da...| null|                     0|                 12|      15|            13|               19|Black|  2013-01-02|2199-12-31|      S|\n",
      "|       3|         21|DBA joke mug - yo...| null|                     0|                 12|      15|            13|               19|Black|  2013-01-02|2199-12-31|      S|\n",
      "|       3|         23|DBA joke mug - it...| null|                     0|                 12|      15|            13|               19|Black|  2013-01-02|2199-12-31|      S|\n",
      "+--------+-----------+--------------------+-----+----------------------+-------------------+--------+--------------+-----------------+-----+------------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productos = productos.withColumn('FechaInicial',lit('2013-01-02'))\n",
    "productos = productos.withColumn('FechaFinal',lit('2199-12-31'))\n",
    "productos = productos.withColumn('Version', lit(1))\n",
    "productos.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga (inicial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGUE\n",
    "guardar_db(dest_db_connection_string, productos,'Estudiante_43.ProductoHistoria', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 3: Actualizaciones de productos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Borrar\n",
    "# Primero: sacamos productos a parte\n",
    "import pandas as pd\n",
    "import random\n",
    "df = pd.read_csv('.\\CSV modificados\\CopiaProductosTransaccional.csv')\n",
    "min_PU, max_PU = df['PrecioUnitario'].min(), df['PrecioUnitario'].max() \n",
    "nuevos.....\n",
    "#nuevos = df.sample(frac = 0.23)\n",
    "#df = df[~df.index.isin(nuevos.index)]\n",
    "\n",
    "cambios = df.sample(frac = 0.23)\n",
    "no_cambios = df[~df.index.isin(cambios.index)]\n",
    "no_cambios = no_cambios.sample(frac = 0.36)\n",
    "cambios['PrecioUnitario'] = cambios['PrecioUnitario'].apply(lambda x : random.randint(min_PU, max_PU))\n",
    "\n",
    "print(cambios.shape, no_cambios.shape)\n",
    "df = pd.concat([cambios, no_cambios])\n",
    "df.to_csv('.\\CSV modificados\\ReporteNum2_Productos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "productosReporte = obterner_dataframe_desde_csv('.\\ReporteNum2_Productos.csv', ',')\n",
    "sql_productos = '''(SELECT * FROM Estudiante_43.ProductoHistoria) AS Temp_productos'''\n",
    "\n",
    "productos = obtener_dataframe_de_bd(source_db_connection_string, sql_productos, db_user, db_psswd)\n",
    "productosReporte.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la bodega de datos (base de datos multidimensional), se tienen varias versiones para un mismo Producto. Sin embargo, para el manejo de historia tipo 2 sólo se necesita la última versión, se utiliza una función Window de PySpark para extraer únicamente la última versión por cada llave natural de un Producto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se utiliza un Window para extraer del dataframe de Productos, por cada ID_Producto, la versión mayor.\n",
    "#Es decir, por cada llave natural, extrae la última versión.\n",
    "window = Window.partitionBy(productos['ID_producto_T']).orderBy(productos['Version'].desc())\n",
    "productos = productos.select('*', f.rank().over(window).alias('rank')).filter(f.col('rank') == 1) \n",
    "productos = productos.drop('rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación\n",
    "hacemos un LEFT JOIN para unir los productos de la tabla ProductoHistoria y los productos del reporte, de manera que se mantienen todos los registros del DataFrame con historia (La primera vez tiene la información cargada en el primer cargue y por ende todas las versiones tienen el valor de 1, FechaInicial 2013-01-02 y FechaFinal 2199-12-31)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO revisar lo de las llaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renombra columnas del DataWarehouse, poniendo sujifo _DWH para evitar confusiones\n",
    "productos = productos.selectExpr('Version as Version_DWH', 'PrecioUnitario as PrecioUnitario_DWH', 'FechaInicio as FechaInicio_DWH', 'FechaFinal as FechaFinal_DWH')\n",
    "\n",
    "#Realiza left join entre stockitems que se están procesando y stockitems en el DW.\n",
    "productos = productos.join(productosReporte.withColumnRenamed('ID_Producto', 'ID_Producto_T'), how = 'left', on ='ID_Producto_T')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar una nueva variable \"ETIQUETA\" que nos indica el tipo de registro para los productos del reporte:\n",
    "- **-1** que representa \"cambios\": un registro que ya existe en ProductoHistoria y que cambio el valor de precio unitario\n",
    "- **0** que representa \"sin cambios\": un registro que ya existe en ProductoHistoria y que no cambio su precio unitario\n",
    "- **1** que representa \"nuevo\": un registro que no existia en ProductoHistoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "productos = productos.withColumn('ETIQUETA', \n",
    "    f.when(productos['ID_Producto_DWH'].isNull(), 1)\\\n",
    "    .when((productos['ID_Producto_DWH'].isNotNull()) \n",
    "          & ((productos['PrecioUnitario'] == productos['PrecioUnitario_DWH']) \n",
    "              | (productos['PrecioUnitario'].isNull() & productos['PrecioUnitario_DWH'].isNull())\n",
    "          ), 0)\\\n",
    "    .otherwise(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los registros que no han cambiado (ETIQUETA = 0), no deben mantenerse en el DataFrame, por lo que se excluyen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "productos = productos.where(df['ETIQUETA'] != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es necesario insertar nuevos registros al Data Warehouse y, por lo tanto, estos necesitan un nuevo id. Para crear el id, primero se encuentra el id máximo y, paso seguido, se utiliza el máximo junto con la función de PySpark monotonically_increasing_id para crear un nuevo id único para cada uno de los registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encontar llave máxima\n",
    "max_key = productos.agg({\"ID_Producto_DWH\": \"max\"}).collect()[0][0]\n",
    "    \n",
    "if max_key is None:\n",
    "    max_key = 1\n",
    "    \n",
    "#Nuevos ids\n",
    "productos = productos.withColumn('new_id', f.monotonically_increasing_id() + max_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, se procede a asignar la versión, fecha de inicio y fecha de vencimiento o final a los registros de acuerdo con las categorías previamente determinadas. Note que los registros que cambiaron deben duplicarse, esto se debe a que una copia del registro se debe usar para actualizar FechaInicial en el Data Warehouse y la otra se utiliza para insertar la última versión del registro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caso 1: Los registros son nuevos (i.e, los registros transaccionales nuevos)\n",
    "productos = productos.withColumn('Version_DWH', f.when(productos['ETIQUETA'] == 1, 1).otherwise(productos['Version_DWH']))\n",
    "productos = productos.withColumn('FechaInicio_DWH', f.when(productos['ETIQUETA'] == 1, f.current_date()).otherwise(productos['FechaInicio_DWH']))\n",
    "productos = productos.withColumn('FechaFinal_DWH', f.when(productos['ETIQUETA'] == 1, f.to_date(f.lit('2199-12-31'), 'yyyy-MM-dd')).otherwise(f.current_date()))\n",
    "productos = productos.withColumn('aInsertar', f.when(productos['ETIQUETA'] == 1, 1).otherwise(0))\n",
    "    \n",
    "#caso -1: Los registros ya existian pero tenian cambios\n",
    "# 1.1 Es necesario editar existentes: poner FechaFinal_DWH como fecha actual. Después, actualizar en la base de datos (NO INSERTAR)\n",
    "# 1.2 Es necesario crear una nueva fila, identica a anterior con los siguientes ajustes: version = version + 1, FechaInicio_DWH = hoy, FechaFinal_DWH = 2199-12-31\n",
    "productos_dup = productos.where(df['ETIQUETA'] == 1)\n",
    "#1.2\n",
    "productos_dup = productos_dup.withColumn('Version_DWH', productos_dup['Version_DWH'] + 1)\n",
    "productos_dup = productos_dup.withColumn('FechaInicio_DWH', f.current_date())\n",
    "productos_dup = productos_dup.withColumn('FechaFinal_DWH', f.to_date(f.lit('2199-12-31'), 'yyyy-MM-dd'))\n",
    "productos_dup = productos_dup.withColumn('aInsertar',f.lit(1))\n",
    "    \n",
    "    \n",
    "# Unir los DataFrames con los registros nuevos (caso -1) y los duplicados (caso 1)\n",
    "productos = productos.union(productos_dup)\n",
    "    \n",
    "#Eliminar la columna 'ETIQUETA' que no se persiste en la base de datos\n",
    "productos = productos.drop('ETIQUETA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se divide el DataFrame en dos, una parte conteniendo los registros a actualizarse y otra los registros a insertar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Persiste DF para obligar ejecución de grafo de computación. TODO: Revisar docs\n",
    "productos.persist()\n",
    "productos = productos.selectExpr('new_id as ID_Producto_DWH',  'NombreProducto', 'Necesita_refrigeracion', 'Dias_tiempo_entrega', 'Impuesto', \\\n",
    "                                 'PrecioUnitario', 'PrecioRecomendado','ID_Color', 'Color', 'Marca', 'ID_PRODUCTO_T', 'Version_DWH as Version',\\\n",
    "                                 'FechaInicio_DWH as FechaInicio', 'FechaFinal_DWH as FechaFinal')\n",
    "#Separar en registros que se deben insertar y registros que se deben actualizar.\n",
    "productos_inserts = productos.where(df['aInsertar'] == 1)\n",
    "productos_updates = productos.where(df['aInsertar'] == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga (Incremental)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERTAMOS REGISTROS: TODO...cambiar por funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guardar_db(dest_db_connection_string, productos_inserts,'Estudiante_43.ProductoHistoria', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La actualización de registros, es un poco más compleja, puesto que Spark no lo soporta. Por lo tanto, el proceso se realiza en la base de datos para lo cual se procede a insertar los registros en una tabla auxiliar (Stockitem_Historia_Update), que contiene las columnas ID_Producto_T y FechaFinal y después, se ejecuta una sentencia SQL update para actualizar la tabla de la dimensión Producto a partir de esta tabla auxiliar, cuyos registros son borrados al final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insertar en tabla auxiliar (ProductoHistoria_Update)\n",
    "guardar_db(dest_db_connection_string, productos_updates,'Estudiante_43.ProductoHistoria_Update', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecutar sentencia SQL UPDATE\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  passwd=\"\",\n",
    "  database=\"WWImportersDWH\",\n",
    ")\n",
    "cur = mydb.cursor()\n",
    "\n",
    "print(mydb)\n",
    "res = cur.execute(\"\"\"\n",
    "UPDATE stockItem_Historia \n",
    "INNER JOIN Stockitem_Historia_Update AS Table_B \n",
    "ON stockItem_Historia.Stock_Item_Key=Table_B.Stock_Item_Key\n",
    "SET stockItem_Historia.Date_to=Table_B.date_to;\n",
    "\"\"\")\n",
    "res = cur.execute(\"\"\"TRUNCATE TABLE Stockitem_Historia_Update;\"\"\")\n",
    "mydb.commit()\n",
    "cur.close()\n",
    "mydb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BLOQUE 5: Dimensión cliente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_categoriasCliente = '''(SELECT ID_Categoria, NombreCategoria FROM WWImportersTransactional.CategoriasCliente) AS Temp_categoriasclientes'''\n",
    "sql_gruposCompra = '''(SELECT ID_GrupoCompra, NombreGrupoCompra FROM WWImportersTransactional.GruposCompra) AS Temp_gruposcompra'''\n",
    "sql_clientes = '''(SELECT ID_Cliente AS ID_Cliente_T, Nombre, ClienteFactura, ID_Categoria, ID_GrupoCompra, ID_CiudadEntrega, LimiteCredito, FechaAperturaCuenta, DiasPago FROM WWImportersTransactional.Clientes) AS Temp_clientes'''\n",
    "\n",
    "categoriasCliente = obtener_dataframe_de_bd(source_db_connection_string, sql_categoriasCliente, db_user, db_psswd)\n",
    "gruposCompra = obtener_dataframe_de_bd(source_db_connection_string, sql_gruposCompra, db_user, db_psswd)\n",
    "clientes = obtener_dataframe_de_bd(source_db_connection_string, sql_clientes, db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+----------+--------------------+--------------+----------------+-------------+-------------------+--------+-----------------+---------------+\n",
      "|ID_Categoria|ID_GrupoCompra|ID_Cliente|              Nombre|ClienteFactura|ID_CiudadEntrega|LimiteCredito|FechaAperturaCuenta|DiasPago|NombreGrupoCompra|NombreCategoria|\n",
      "+------------+--------------+----------+--------------------+--------------+----------------+-------------+-------------------+--------+-----------------+---------------+\n",
      "|           3|             2|       601|Wingtip Toys (Rut...|           401|           29887|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       600|Wingtip Toys (Car...|           401|            5407|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       599|Wingtip Toys (Dic...|           401|            9077|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       598|Wingtip Toys (Kap...|           401|           17340|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       597|Wingtip Toys (Hay...|           401|           14965|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       596|Wingtip Toys (Cos...|           401|            7695|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       595|Wingtip Toys (Acc...|           401|              54|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       594|Wingtip Toys (Mar...|           401|           20924|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       593|Wingtip Toys (Cuy...|           401|            8296|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       592|Wingtip Toys (Dun...|           401|            9549|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       591|Wingtip Toys (Ida...|           401|           16382|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       590|Wingtip Toys (McA...|           401|           21405|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       589|Wingtip Toys (Alc...|           401|             357|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       588|Wingtip Toys (Oak...|           401|           24988|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       587|Wingtip Toys (Pla...|           401|           27236|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       586|Wingtip Toys (Lyn...|           401|           20376|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       585|Wingtip Toys (Tow...|           401|           34285|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       584|Wingtip Toys (Cac...|           401|            4840|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       583|Wingtip Toys (Kni...|           401|           17935|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "|           3|             2|       582|Wingtip Toys (Mon...|           401|           22814|         null|2013-01-01 00:00:00|       7|     Wingtip Toys|   Novelty Shop|\n",
      "+------------+--------------+----------+--------------------+--------------+----------------+-------------+-------------------+--------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TRANSFORMACION\n",
    "clientes = clientes.join(gruposCompra, how = 'inner', on = 'ID_GrupoCompra')\n",
    "clientes = clientes.join(categoriasCliente, how = 'inner', on = 'ID_Categoria')\n",
    "clientes = clientes.withColumn('ID_Cliente_DWH', f.monotonically_increasing_id() + 1)\n",
    "clientes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGUE\n",
    "guardar_db(dest_db_connection_string,clientes,'Estudiante_43.Cliente', db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique los resultados usando MySQL Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BLOQUE 6: Hecho orden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_ordenes = '''(SELECT * FROM WWImportersTransactional.Ordenes) AS Temp_ordenes'''\n",
    "sql_detallesOrdenes = '''(SELECT * FROM WWImportersTransactional.DetallesOrdenes) AS Temp_detallesordenes'''\n",
    "ordenes = obtener_dataframe_de_bd(source_db_connection_string, sql_ordenes, db_user, db_psswd)\n",
    "detallesOrdenes = obtener_dataframe_de_bd(source_db_connection_string, sql_detallesOrdenes, db_user, db_psswd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformación\n",
    "Estas son las respuestas de Wide World Importers a los conclusiones obtenidas en el entendimiento de los datos:\n",
    "- Diferencia entre cantidad y cantidad seleccionada es X\n",
    "- La regla de negocio \"La tasa de impuesto es de 10% o 15%\" es correcta, pero habian errores en la tabla original, que fueron corregidos. \n",
    "- Para la segunda regla de negocio: \"Son 73.595 órdenes detalladas en 231.412 lineas de detalle de órdenes realizadas desde 2013\", si faltaban datos, los cuales fueron completados, y nos dicen que en cuanto a consistencia ellos revisaron las tablas e hicieron correcciones, pero que los duplicados completos de ordenes los eliminemos\n",
    "- \"El formato de fechas manejado es YYYY-MM-DD HH:MM:SS si tienen hora, minutos y segundos. De lo contrario el formato es YYYY-MM-DD\": En cuanto a formatos de fechas estan de acuerdo con que los estandarizemos y el formato sea el especificado en la regla\n",
    "- Para las descripciones de productos que eran \"a\", se actualizaron a los valores reales. \n",
    "- Se pueden eliminar las columnas Comenarios, Instrucciones_de_entrega y comentarios_internos porque estan vacias. \n",
    "- A pesar de estar en un proceso de mejorar la calidad de los datos y mantener los nulos nos ayudaría a reflejar esa calidad, de la mano con el grupo de analitica de WWI se decide imputar por la media el valor extremo de la variable Cantidad\n",
    "- Para las ordenes las columnas Seleccionado_por_ID_de_persona, ID_de_pedido_pendiente, Seleccion_completada_cuando, y para las columnas Seleccion_completada_cuando de la tabla detalles de ordenes, se decide mantener los valores vacíos, sin embargo para la variable Precio_unitario el negocio reviso y complemento los valores faltantes\n",
    "\n",
    "Las tablas usadas en el tutorial de entendimiento de datos estaran disponibles para su revision con los siguientes nombres: OrdenesCopia y DetallesOrdenesCopia. \n",
    "\n",
    "Para este tutorial vamos a trabajar con unas tablas que dadas las conclusiones del tutorial de entendimiento, WWImporters revisó los datos originales, creo tablas y las llamo \"Ordenes\" y \"DetallesOrdenes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se hace una verificación de los valores de la tasa de impuesto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|Tasa_de_impuesto|\n",
      "+----------------+\n",
      "|              10|\n",
      "|              15|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detallesOrdenes.select(\"Tasa_de_impuesto\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se hace una verificación del rango de fechas disponible en los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|min(Fecha_de_pedido)|\n",
      "+--------------------+\n",
      "|          2013-01-01|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordenes.agg({\"Fecha_de_pedido\": \"min\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina columnas Comenarios, Instrucciones_de_entrega y comentarios_internos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenes = ordenes.drop(*[\"Comentarios\", \"Instrucciones_de_entrega\",\"comentarios_internos\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se eliminan duplicados totales de ordenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107707, 93629)\n"
     ]
    }
   ],
   "source": [
    "print((ordenes.count(),ordenes.distinct().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenes = ordenes.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93629, 93629)\n"
     ]
    }
   ],
   "source": [
    "print((ordenes.count(),ordenes.distinct().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se hace verificación de consistencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#consistencia: revisar genially: definicion de consistencia\n",
    "ids_ordenes = set([x.ID_de_pedido for x in ordenes.select('ID_de_pedido').collect()])\n",
    "ids_detalles = set([x.ID_de_pedido for x in detallesOrdenes.select('ID_de_pedido').collect()])\n",
    "\n",
    "len(ids_ordenes-ids_detalles), len(ids_detalles-ids_ordenes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente código para el manejo de fechas, pasamos del formato MM dd,YYYY al formato establecido en la regla de negocio<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(93629, 93629)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRANSFORMACION\n",
    "regex = \"([0-2]\\d{3}-(0[1-9]|1[0-2])-(0[1-9]|[1-2][0-9]|3[0-1]))\"\n",
    "cumplenFormato = ordenes.filter(ordenes[\"Fecha_de_pedido\"].rlike(regex))\n",
    "noCumplenFormato = ordenes.filter(~ordenes[\"Fecha_de_pedido\"].rlike(regex))\n",
    "print(noCumplenFormato.count(), cumplenFormato.count())\n",
    "print(noCumplenFormato.show(5))\n",
    "noCumplenFormato = noCumplenFormato.withColumn('Fecha_de_pedido', f.udf(lambda d: datetime.strptime(d, '%b %d,%Y').strftime('%Y-%m-%d'), t.StringType())(f.col('Fecha_de_pedido')))\n",
    "ordenes = noCumplenFormato.union(cumplenFormato)\n",
    "noCumplenFormato.count(), ordenes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descripciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+-----------+-----------+---------------+--------+---------------+----------------+---------------------+---------------------------+\n",
      "|Detalle_orden_ID|ID_de_pedido|ID_Producto|Descripcion|ID_Tipo_Paquete|Cantidad|Precio_unitario|Tasa_de_impuesto|Cantidad_seleccionada|Seleccion_completada_cuando|\n",
      "+----------------+------------+-----------+-----------+---------------+--------+---------------+----------------+---------------------+---------------------------+\n",
      "+----------------+------------+-----------+-----------+---------------+--------+---------------+----------------+---------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "detallesOrdenes.where(length(col(\"Descripcion\")) <= 10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputar valor maximo de cantidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Cantidad=360)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detallesOrdenes.select('Cantidad').sort(col(\"Cantidad\").desc()).collect()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "detallesOrdenes = detallesOrdenes.replace( 10000000, 360, 'Cantidad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Cantidad=360)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detallesOrdenes.select('Cantidad').sort(col(\"Cantidad\").desc()).collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se unen los dos dataframes, se verifica que no haya duplicados y si los hay se eliminan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenes = ordenes.join(detallesOrdenes, how = 'inner', on = 'ID_de_pedido')\n",
    "ordenes = ordenes.withColumn('Valor_total',col('Precio_unitario')*col('Cantidad'))\n",
    "ordenes = ordenes.withColumn('Impuestos',col('Valor_total')*col('Tasa_de_impuesto'))\n",
    "ordenes = ordenes.selectExpr('ID_de_pedido as ID_de_pedido_T','ID_Producto','Fecha_de_pedido','ID_de_cliente','ID_de_vendedor','ID_Tipo_Paquete','Cantidad','Valor_total', 'Impuestos')\n",
    "\n",
    "print((ordenes.count(),ordenes.distinct().count()))\n",
    "\n",
    "ordenes = ordenes.drop_duplicates()\n",
    "ordenes = ordenes.withColumnRename('ID_de_pedido_DWH', f.monotonically_increasing_id() + 1)\n",
    "ordenes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CARGUE\n",
    "inferior = 0\n",
    "superior = 999\n",
    "j=0\n",
    "total = ordenes.count()/1000\n",
    "print(total)\n",
    "collected = ordenes.collect()\n",
    "while j<total:\n",
    "    if j%50==0:\n",
    "        print(j)\n",
    "    j += 1\n",
    "    aux = spark.createDataFrame(collected[inferior:superior],ordenes.columns)\n",
    "    guardar_db(dest_db_connection_string, aux,'Estudiante_43.HechoOrden', db_user, db_psswd)\n",
    "    inferior+=1000\n",
    "    superior+=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifique los resultados usando MySQL Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultado de consultas\n",
    "Corresponde a las consultas realizadas sobre las tablas, para mostrar el estado final de las tablas pobladas como resultado del proceso de ETL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tarea ETL\n",
    "Espacio para desarrollar la tarea planteada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cierre\n",
    "Completado este tutorial, sabe cómo configurar y realizar ETLs con historia en PySpark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Información adicional\n",
    "\n",
    "Si quiere conocer más sobre PySpark la guía más detallada es la documentación oficial, la cual puede encontrar acá: https://spark.apache.org/docs/latest/api/python/index.html <br>\n",
    "Para ir directamente a la documentación de PySpark SQL, donde está la información sobre los DataFrames, haga clic en este enlace: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html <br>\n",
    "\n",
    "Para saber más sobre las técnicas de manejo de historia, consulte el libro <i>The  Data Warehouse Toolkit</i> de Ralph Kimball y Margy Ross,que podrá encontrar en la biblioteca de la universidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preguntas frecuentes\n",
    "\n",
    "- Si al intentar escribir un <i>dataframe</i> obtiene un error en el formato: \n",
    "    ```\n",
    "    path file:<PATH>/dw/<PATH> already exists.;\n",
    "    ```\n",
    "    Borre la carpeta indicada en el error y vuelva a intentar.\n",
    "\n",
    "- Si al ejecutar su código obtiene el error: \n",
    "    ```\n",
    "    ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=tutorial ETL PySpark, master=local) created by __init__ at <ipython-input-4-64455da959dd>:92 \n",
    "\n",
    "    ```\n",
    "    reinicie el kernel del notebook y vuelva a intentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
